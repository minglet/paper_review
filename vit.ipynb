{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat # einops 직관적으로 사용할 수 있는 차원관리 package\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project input to Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8, 3, 224, 224])\n",
      "patches: torch.Size([8, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "x = torch.randn(8, 3, 224, 224) # (batch_size, channel, height, width)\n",
    "print(f'x: {x.shape}')\n",
    "\n",
    "# b c (h s1) (w s2) # height, width를 각각 patch_size로 나누라는 말\n",
    "# b (h w) (s1 s2 c) # batch_size는 그대로 두고 h*w로 펼쳐서 1차원으로 만들고(14*14=196) s1*s2*3을 펼쳐서 (16*16*3)이 된다. \n",
    "patch_size = 16\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', \n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print(f\"patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "in_channels = 3\n",
    "emb_size = 768 # channel * patch_size * patch_size\n",
    "\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size), # [-1, 786, 14, 14]\n",
    "    Rearrange('b e (h) (w) -> b (h w) e') # [-1, 14*14, 786]\n",
    ")\n",
    "summary(projection, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding\n",
    "- patches에 class token & positional embedding을 넣어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection X shape:  torch.Size([8, 196, 768])\n",
      "Class Shape:  torch.Size([1, 1, 768]) Position Shape:  torch.Size([197, 768])\n",
      "Repeated Class Shape:  torch.Size([8, 1, 768])\n",
      "output:  torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "# 이미지를 패치 사이즈로 나누고 flatten\n",
    "projection_x = projection(x)\n",
    "print('Projection X shape: ', projection_x.shape)\n",
    "\n",
    "# cls_token과 pos encoding prameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1,1,emb_size)) # cls_token은 embedding 갯수만큼 생성\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size)**2 + 1, emb_size)) # R(N+1 * D) # N+1인 이유는 cls_token 때문에\n",
    "print('Class Shape: ', cls_token.shape, 'Position Shape: ', positions.shape)\n",
    "\n",
    "# cls_token을 반복하여 배치사이즈의 크기와 맞춰줌\n",
    "batch_size = 8\n",
    "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "print('Repeated Class Shape: ', cls_tokens.shape)\n",
    "\n",
    "# cls_token과 projected_x를 concatenate\n",
    "cat_x = torch.cat([cls_tokens, projection_x], dim=1) # [8, 197, 768]\n",
    "\n",
    "# position encoding을 더해줌\n",
    "cat_x += positions # cat_x의 요소에 positions 값을 넣어서 더해줌 # shape에는 변함 없음!\n",
    "print('output: ', cat_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat shape:  torch.Size([4, 9, 16])\n"
     ]
    }
   ],
   "source": [
    "# 어떻게 concatenate되는지 확인해보기\n",
    "tensor_1 = nn.Parameter(torch.randn(4, 1, 16))\n",
    "tensor_2 = nn.Parameter(torch.randn(4, 8, 16))\n",
    "cat_tensor = torch.cat([tensor_1, tensor_2], dim=1)\n",
    "print('cat shape: ', cat_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Class 형태로 만들어주기\n",
    "class PathEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels:int=3, patch_size:int=16, emb_size:int=768, img_size:int=224) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**2 + 1, emb_size))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape # batch_size 선언\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.positions\n",
    "        return x\n",
    "\n",
    "PE = PathEmbedding()\n",
    "summary(PE, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "- 패치들에 대해서 self attention 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([8, 197, 768])\n",
      "shape:  torch.Size([8, 8, 197, 96]) torch.Size([8, 8, 197, 96]) torch.Size([8, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "# q, k, v 정의하기\n",
    "keys = nn.Linear(emb_size, emb_size)\n",
    "queries = nn.Linear(emb_size, emb_size)\n",
    "values = nn.Linear(emb_size, emb_size)\n",
    "print(keys, queries, values)\n",
    "\n",
    "x = PE(x)\n",
    "print(queries(x).shape) # batch, n, emb_size\n",
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads) # emb_size를 h*d 형태로 만들면 # h=num_heads(8), d=emb_size/h(96)\n",
    "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "print('shape: ', queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy:  torch.Size([8, 8, 197, 197])\n",
      "att:  torch.Size([8, 8, 197, 197])\n",
      "out:  torch.Size([8, 8, 197, 96])\n",
      "out_1:  torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Queries * Keys\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print('energy: ', energy.shape)\n",
    "\n",
    "# Get Attention Score\n",
    "scaling = emb_size ** (1/2)\n",
    "att = F.softmax(energy/scaling, dim=1)\n",
    "print('att: ', att.shape)\n",
    "\n",
    "# Attention Score * values\n",
    "out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "print('out: ', out.shape)\n",
    "\n",
    "# Reagrrange to emb_size\n",
    "out_1 = rearrange(out, \"b h n d -> b n (h d)\") # 처음 input(x)과 같은 shape으로 반환 \n",
    "print('out_1: ', out_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
